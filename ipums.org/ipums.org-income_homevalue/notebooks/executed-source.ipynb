{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Title"
    ]
   },
   "source": [
    "# Senior Income and Home Value Distributions For San Diego County"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Description"
    ]
   },
   "source": [
    "This package extracts the home value and household income for households in San DIego county with one or more household members aged 65 or older. .  The base data is from the 2015 5 year PUMS sample, from IPUMS<sup>[1](#ipums)</sup>. The dataset variables used are: HHINCOME and VALUEH. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extract is intended for analysis of senior issues in San Diego County, so the record used are further restricted with these filters: \n",
    "\n",
    "* WHERE AGE > = 65\n",
    "* HHINCOME < 9999999\n",
    "* VALUEH < 9999999 \n",
    "* STATEFIP = 6 \n",
    "* COUNTYFIPS = 73\n",
    "\n",
    "The limits on the HHINCOME and VALUEH variables eliminate top coding. \n",
    "\n",
    "This analysis used the IPUMS [(ipums)](#ipums) data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext metatab\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%mt_lib_dir lib\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "import metatab as mt\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "import sqlite3\n",
    "\n",
    "import statsmodels as sm\n",
    "from statsmodels.nonparametric.kde import KDEUnivariate\n",
    "from scipy import integrate, stats\n",
    "\n",
    "from incomedist import * \n",
    "from multikde import MultiKde \n",
    "\n",
    "plt.rcParams['figure.figsize']=(6,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%metatab \n",
    "Origin: ipums.org\n",
    "Dataset: income_homevalue\n",
    "Identifier: b407e5af-cc23-431d-a431-15c202ec0c3b\n",
    "Name: ipums.org-income_homevalue-4\n",
    "Version: 4\n",
    "\n",
    "Section: Contacts\n",
    "Wrangler: Eric Busboom\n",
    "Wrangler.Email: eric@civicknowledge.com\n",
    "    \n",
    "Section: Bibliography\n",
    "Citation: ipums\n",
    "Citation.Type: dataset\n",
    "Citation.Author: Steven Ruggles; Katie Genadek; Ronald Goeken; Josiah Grover; Matthew Sobek\n",
    "Citation.Title: Integrated Public Use Microdata Series\n",
    "Citation.Year: 2017\n",
    "Citation.Publisher: University of Minnesota\n",
    "Citation.Version: 7.0 \n",
    "Citation.AccessDate: 20170718\n",
    "Citation.Url: https://usa.ipums.org/usa/index.shtml\n",
    "Citation.Doi: https://doi.org/10.18128/D010.V7.0\n",
    "        \n",
    "Citation: bordley\n",
    "Citation.Type: article\n",
    "Citation.Author: Robert F. Bordley; James B. McDonald; Anand Mantrala\n",
    "Citation.Title: Something New, Something Old: Parametric Models for the Size of Distribution of Income\n",
    "Citation.Year: 1997\n",
    "Citation.Month: June\n",
    "Citation.Journal: Journal of Income Distribution\n",
    "Citation.Volume: 6 \n",
    "Citation.Number: 1\n",
    "Citation.Pages: 5-5\n",
    "Citation.Url: https://ideas.repec.org/a/jid/journl/y1997v06i1p5-5.html\n",
    "        \n",
    "Citation: mcdonald\n",
    "Citation.Type: article \n",
    "Citation.Author: McDonald, James B.;  Mantrala, Anand\n",
    "Citation.Title: The distribution of personal income: Revisited\n",
    "Citation.Journal: Journal of Applied Econometrics\n",
    "Citation.Volume: 10\n",
    "Citation.Number: 2\n",
    "Citation.Publisher: Wiley Subscription Services, Inc., A Wiley Company\n",
    "Citation.Issn: 1099-1255\n",
    "Citation.Doi: 10.1002/jae.3950100208\n",
    "Citation.Pages: 201--204,\n",
    "Citation.Year: 1995\n",
    "    \n",
    "Citation: majumder\n",
    "Citation.Type: article \n",
    "Citation.Author: Majumder, Amita; Chakravarty, Satya Ranjan\n",
    "Citation.Title: Distribution of personal income: Development of a new model and its application to U.S. income data\n",
    "Citation.Journal: Journal of Applied Econometrics\n",
    "Citation.Volume: 5\n",
    "Citation.Number: 2\n",
    "Citation.Publisher: Wiley Subscription Services, Inc., A Wiley Company\n",
    "Citation.Issn: 1099-1255\n",
    "Citation.Doi: 10.1002/jae.3950050206\n",
    "Citation.Pages: 189--196\n",
    "Citation.Year: 1990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create a sample of a SQL database, so we can edit the schema. \n",
    "# Run the cell once to create the schema, then edit the schema and run it \n",
    "# again to build the database. \n",
    "\n",
    "fn='/Volumes/Storage/Downloads/usa_00005.csv'\n",
    "if [ ! -e schema.sql ]\n",
    "then\n",
    "    head -100 $fn > sample.csv\n",
    "    sqlite3 --csv ipums.sqlite '.import sample.csv ipums'\n",
    "    sqlite3 ipums.sqlite  .schema > schema-orig.sql\n",
    "    sqlite3 -header ipums.sqlite \"select * from ipums limit 2\"  > sample.sql # Show a sample of data\n",
    "    rm ipums.sqlite\n",
    "fi    \n",
    "\n",
    "if [ -e schema.sql -a \\( ! -e ipums.sqlite \\) ]\n",
    "then\n",
    "    sqlite3 ipums.sqlite < schema.sql\n",
    "    sqlite3 --csv ipums.sqlite \".import $fn ipums\"\n",
    "    # Create some indexes to speed up queries\n",
    "    sqlite3 ipums.sqlite \"CREATE INDEX IF NOT EXISTS state_idx ON ipums (STATEFIP)\"\n",
    "    sqlite3 ipums.sqlite \"CREATE INDEX IF NOT EXISTS county_idx ON ipums (COUNTYFIPS)\"\n",
    "    sqlite3 ipums.sqlite \"CREATE INDEX IF NOT EXISTS state_county_idx ON ipums (STATEFIP, COUNTYFIPS)\"\n",
    "    \n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Data\n",
    "\n",
    "The PUMS data is a sample, so both household and person records have weights. We use those weights to replicate records. We are not adjusting the values for CPI, since we don't have a CPI for 2015, and because the medians for income comes out pretty close to those from the 2015 5Y ACS. \n",
    "\n",
    "The `HHINCOME` and `VALUEH` have the typical distributions for income and home values, both of which look like Poisson distributions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the weights for the whole file to see if they sum to the number\n",
    "# of households and people in the county. They don't, but the sum of the weights for households is close, \n",
    "# 126,279,060 vs about 116M housholds\n",
    "con = sqlite3.connect(\"ipums.sqlite\")\n",
    "wt = pd.read_sql_query(\"SELECT YEAR, DATANUM, SERIAL, HHWT, PERNUM, PERWT FROM ipums \"\n",
    "                       \"WHERE PERNUM = 1 AND YEAR = 2015\", con)\n",
    "\n",
    "wt.drop(0, inplace=True)\n",
    "\n",
    "nd_s = wt.drop_duplicates(['YEAR', 'DATANUM','SERIAL'])\n",
    "country_hhwt_sum = nd_s[nd_s.PERNUM == 1]['HHWT'].sum()\n",
    "\n",
    "len(wt), len(nd_s), country_hhwt_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# PERNUM = 1 ensures only record for each household \n",
    "\n",
    "con = sqlite3.connect(\"ipums.sqlite\")\n",
    "senior_hh = pd.read_sql_query(\n",
    "                       \"SELECT DISTINCT SERIAL, HHWT, PERWT, HHINCOME, VALUEH \"\n",
    "                       \"FROM ipums \"\n",
    "                       \"WHERE AGE >= 65 \" \n",
    "                       \"AND HHINCOME < 9999999 AND VALUEH < 9999999 \"\n",
    "                       \"AND STATEFIP = 6 AND COUNTYFIPS=73 \", con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we're doing a probabilistic simulation, the easiest way to deal with the weight is just to repeat rows. \n",
    "# However, adding the weights doesn't change the statistics much, so they are turned off now, for speed. \n",
    "\n",
    "def generate_data():\n",
    "    \n",
    "    for index, row in senior_hh.drop_duplicates('SERIAL').iterrows():\n",
    "        #for i in range(row.HHWT):\n",
    "        yield (row.HHINCOME, row.VALUEH)\n",
    "  \n",
    "incv = pd.DataFrame(list(generate_data()), columns=['HHINCOME', 'VALUEH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.jointplot(x=\"HHINCOME\", y=\"VALUEH\", marker='.', scatter_kws={'alpha': 0.1}, data=incv, kind='reg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedure\n",
    "\n",
    "After extracting the data for HHINCOME and VALUEH, we rank both values and then quantize the rankings into 10 groups, 0 through 9, `hhincome_group` and `valueh_group`. The `HHINCOME` variable correlates with `VALUEH` at .36, and the quantized rankings `hhincome_group` and `valueh_group` correlate at .38.\n",
    "\n",
    "Initial attempts were made to fit curves to the income and home value distributions, but it is very difficult to find well defined models that fit real income distributions. Bordley [(bordley)](#bordley) analyzes the fit for 15 different distributions, reporting success with variations of the generalized beta distribution, gamma and Weibull. Majumder [(majumder)](#majumder) proposes a four parameter model with variations for special cases. None of these models were considered well established enough to fit within the time contraints for the project, so this analysis will use empirical distributions that can be scale to fit alternate parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "incv['valueh_rank'] = incv.rank()['VALUEH']\n",
    "incv['valueh_group'] = pd.qcut(incv.valueh_rank, 10, labels=False )\n",
    "incv['hhincome_rank'] = incv.rank()['HHINCOME']\n",
    "incv['hhincome_group'] = pd.qcut(incv.hhincome_rank, 10, labels=False )\n",
    "incv[['HHINCOME', 'VALUEH', 'hhincome_group', 'valueh_group']] .corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metatab.pands import MetatabDataFrame\n",
    "odf = MetatabDataFrame(incv)\n",
    "odf.name = 'income_homeval'\n",
    "odf.title = 'Income and Home Value Records for San Diego County'\n",
    "odf.HHINCOME.description = 'Household income'\n",
    "odf.VALUEH.description = 'Home value'\n",
    "odf.valueh_rank.description = 'Rank of the VALUEH value'\n",
    "odf.valueh_group.description = 'The valueh_rank value quantized into 10 bins, from 0 to 9'\n",
    "odf.hhincome_rank.description = 'Rank of the HHINCOME value'\n",
    "odf.hhincome_group.description = 'The hhincome_rank value quantized into 10 bins, from 0 to 9'\n",
    "\n",
    "%mt_add_dataframe odf  --materialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we group the dataset by `valueh_group` and collect all of the income values for each group. These groups have different distributions, with the lower numbered group shewing to the left and the higher numbered group skewing to the right. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use these groups in a simulation, the user would select a group for a subject's home value, then randomly select an income in that group. When this is done many times, the original `VALUEH` correlates to the new distribution ( here, as `t_income` ) at .33, reasonably similar to the original correlations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "mk = MultiKde(odf, 'valueh_group', 'HHINCOME')\n",
    "\n",
    "fig,AX = plt.subplots(3, 3, sharex=True, sharey=True, figsize=(15,15))\n",
    "\n",
    "incomes = [30000,\n",
    "          40000,\n",
    "          50000,\n",
    "          60000,\n",
    "          70000,\n",
    "          80000,\n",
    "          90000,\n",
    "          100000,\n",
    "          110000]\n",
    "\n",
    "for mi, ax in zip(incomes, AX.flatten()):\n",
    "    s, d, icdf, g = mk.make_kde(mi)\n",
    "    syn_d = mk.syn_dist(mi, 10000)\n",
    "   \n",
    "    syn_d.plot.hist(ax=ax, bins=40, title='Median Income ${:0,.0f}'.format(mi), normed=True, label='Generated')\n",
    "\n",
    "    ax.plot(s,d, lw=2, label='KDE')\n",
    "    \n",
    "fig.suptitle('Income Distributions By Median Income\\nKDE and Generated Distribution')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scatter matrix show similar structure for `VALUEH` and `t_income`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = incv.copy()\n",
    "t['t_income'] = mk.syn_dist(t.HHINCOME.median(), len(t))\n",
    "t[['HHINCOME','VALUEH','t_income']].corr()\n",
    "\n",
    "sns.pairplot(t[['VALUEH','HHINCOME','t_income']]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulated incomes also have similar statistics to the original incomes. However, the median income is high. In San Diego county, the median household income for householders 65 and older in the 2015 5 year ACS about \\$51K, versus \\$56K here. For home values, the mean home value for 65+ old homeowners is \\$468K in the 5 year ACS, vs \\$510K here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html, HTML\n",
    "display(HTML(\"<h3>Descriptive Stats</h3>\"))\n",
    "t[['VALUEH','HHINCOME','t_income']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"<h3>Correlations</h3>\"))\n",
    "t[['VALUEH','HHINCOME','t_income']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mt_bibliography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new KDE distribution, based on the home values, including only home values ( actually KDE supports ) between $130,000 and $1.5M. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Show"
    ]
   },
   "outputs": [],
   "source": [
    "s,d = make_prototype(incv.VALUEH.astype(float), 130_000, 1_500_000)\n",
    "\n",
    "plt.plot(s,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlay the prior plot with the histogram of the original values. We're using np.histogram to make the histograph, so it appears as a line chart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Show"
    ]
   },
   "outputs": [],
   "source": [
    "v = incv.VALUEH.astype(float).sort_values()\n",
    "#v = v[ ( v > 60000 ) & ( v < 1500000 )]\n",
    "\n",
    "hist, bin_edges = np.histogram(v, bins=100, density=True)\n",
    "\n",
    "bin_middles = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "\n",
    "bin_width = bin_middles[1] - bin_middles[0]\n",
    "\n",
    "assert np.isclose(sum(hist*bin_width),1) # == 1 b/c density==True\n",
    "\n",
    "hist, bin_edges = np.histogram(v, bins=100) # Now, without 'density'\n",
    "\n",
    "# And, get back to the counts, but now on the KDE\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(s,d * sum(hist*bin_width));\n",
    "\n",
    "ax.plot(bin_middles, hist);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show an a home value curve, interpolated to the same values as the distribution. The two curves should be co-incident. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Show"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_compare_curves(p25, p50, p75):\n",
    "    fig = plt.figure(figsize = (8,3))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    sp, dp = interpolate_curve(s, d, p25, p50, p75)\n",
    "\n",
    "    ax.plot(pd.Series(s), d, color='black');\n",
    "    ax.plot(pd.Series(sp), dp, color='red');\n",
    "\n",
    "# Re-input the quantiles for the KDE\n",
    "# Curves should be co-incident\n",
    "plot_compare_curves(2.800000e+05,4.060000e+05,5.800000e+05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, interpolate to the values for the county, which shifts the curve right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Show"
    ]
   },
   "outputs": [],
   "source": [
    "# Values for SD County home values\n",
    "plot_compare_curves(349100.0,485900.0,703200.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of creating an interpolated distribution, then generating a synthetic distribution from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Show"
    ]
   },
   "outputs": [],
   "source": [
    "sp, dp = interpolate_curve(s, d, 349100.0,485900.0,703200.0)\n",
    "v = syn_dist(sp, dp, 10000)\n",
    "\n",
    "plt.hist(v, bins=100);  \n",
    "pd.Series(v).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "epilog": true,
    "mt_materialize": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%mt_materialize /Volumes/Storage/proj/virt/data-projects/sdrdl-data-projects/ipums.org/income_home_value/ipums.org-income_homevalue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "epilog": true,
    "mt_final_metatab": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%mt_show_metatab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "epilog": true,
    "mt_show_libdirs": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%mt_show_libdirs\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}